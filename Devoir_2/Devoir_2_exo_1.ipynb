{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nom du binôme :\n",
    "- Philippe Marcotte, 20124486\n",
    "- Aboubaker Aden Houssein, 20139764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 : Régression linéaire et non linéaire régularisée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Régression linéaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit un problème de régression (apprentissage supervisé) pour lequel on dispose d'un ensemble de données d'entraînement $D_{n}$ = $\\{(x^{(1)},t^{(1)}),...,(x^{(n)},t^{(n)})\\}$ avec $x^{(i)} \\in \\mathbb{R}^{d}$ et $t^{(i)} \\in \\mathbb{R}$. \n",
    "\n",
    "1) Pour la régression, nous choisissons une famille paramétrique de classe de fonctions qui sont les fonctions linéaires. La forme paramétrée de la fonction de prédiction est : $f_{\\theta}(x) = f(x) = w^{T}x + b = \\sum_{i=1}^d w_{i}x_{i} + b$ dont les paramètres à estimer sont : $\\theta = \\{w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}\\}$ donc on aura $d+1$ paramètres à estimer en tout (en les apprenant dans notre ensemble d'apprentissage). Les $w_{i}$ sont les coefficients des _features_ de l'entrée $x : x_{1},...,x_{d}$. \n",
    "\n",
    "2) On choisi également une fonction de coût (erreur quadratique) qui va mésurer la perte (ou l'erreur) faite pour la prédiction d'un exemple $x$ : \n",
    "\n",
    "$L((x,t), f) = L(f(x), t) = (f(x) - t)^{2} = (w^{T}x + b - t)^{2}$ (à noter en termes de notation équivalente : $f(x) = f_{\\theta}(x)$) \n",
    "\n",
    "L'expression mathématique donnant le risque empirique (i.e. que le __risque empirirque__ $\\hat{R}$ sur l'ensemble $D_{n}$ comme étant la somme des pertes sur l'ensemble $D_{n}$ ) :\n",
    "$J(\\theta) = \\hat{R}(f_{\\theta}(x), D_{n}) = \\sum_{i=1}^n L(f_{\\theta}(x)^{(i)}, t^{(i)}) = \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2 $, où $f_{\\theta}(x)^{(i)}$ est la valeur prédite pour le point $x^{(i)}$ et $t^{(i)}$ est la valeur cible correspondant. \n",
    "\n",
    "On sait que d'après le principe de minimisation de risque empirique (MRE), comme le vrai risque $R(f_{\\theta}(x))$ ne peut être calculé car la distribution $P(x,t)$ ayant permis de générer les données est inconnue pour l'algorithme d'apprentissage, on va plutôt chercher à approximer cela en minimisant le risque empirique (MRE: $\\hat{R}(f_{\\theta}(x), D_{n})$) sur la somme des pertes sur le training set $D_{n}$. \n",
    "\n",
    "3) Après avoir choisi : une classe de fonction paramétrique, une fonction de coût pour évaluer nos erreurs de prédiction et défini le risque empirique, nous devons désormais résoudre ce problème d'optimisation (MRE) en trouvant la valeur des paramètres $\\theta$ (i.e. : $\\{w, b\\}$) qui minimise le risque empirique (i.e. les erreurs d'entrainement). Ce problème d'optimisation s'exprime ainsi : \n",
    "$\n",
    "\\begin{equation*}\\{w^{*}, b^{*}\\} = \\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\hat{R}(f_\\theta, D_{n}) = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n L(f_{\\theta}(x)^{(i)}, t^{(i)}) = \\underset{w, b}{\\operatorname{arg min}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2 \\end{equation*}\n",
    "$\n",
    " \n",
    "Ce problème d'optimisation peut être résolu de deux manières : par technique descente de gradient ou de manière analytique (calcul matriciel). \n",
    "Ci-dessous en pose $\\{\\theta_{0}=b, \\theta_{1}=w_{1}, ..., \\theta_{d}=w_{d}\\}$)\n",
    "\n",
    "4) Dans notre cas, on va opter pour la technique d'optimisation par descente du gradient en exprimant l'expression du gradient du risque empirique : \n",
    "$ \n",
    "\\nabla \\hat{R}(f_\\theta, D_{n}) = \\begin{equation*}\\begin{bmatrix} \\frac{\\partial\\hat{R}(f_\\theta, D_{n})}{\\partial b} \\\\ \\frac{\\partial\\hat{R}(f_\\theta, D_{n})}{\\partial w_{1}} \\\\ \\vdots \\\\ \\frac{\\partial\\hat{R}(f_\\theta, D_{n})}{\\partial w_{d}} \\end{bmatrix} = \\begin{bmatrix} \n",
    "\\frac{\\partial}{\\partial b} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2 \\\\ \\frac{\\partial}{\\partial w_{1}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2  \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial w_{d}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2  \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial}{\\partial b} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2 \\\\ \\frac{\\partial}{\\partial w_{1}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2  \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial w_{d}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2  \n",
    "\\end{bmatrix}  = \n",
    "\\begin{bmatrix} \n",
    "2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)}) \\\\ 2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})*x_{1}^{(i)}  \\\\ \\vdots \\\\ 2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})*x_{d}^{(i)}  \n",
    "\\end{bmatrix}  = \n",
    "\\begin{bmatrix} \n",
    "2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)}) \\\\ 2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)})*x_{1}^{(i)}  \\\\ \\vdots \\\\ 2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)})*x_{d}^{(i)}  \n",
    "\\end{bmatrix}\\end{equation*}\n",
    "$\n",
    "\n",
    "5) Les erreurs du modèle (erreur quadratique) sur l'ensemble de données sont celles qui sont faites lorsque des valeurs précises de $\\theta = \\{w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}\\}$ sont choisies. Nous cherchons donc à prendre des valeurs de $\\theta$ qui minimisent le risque empirique. Pour cela le calcul de gradient sur la surface de coût paysage va nous permettre de chercher la valeur optimale de $\\theta^{*}$ qui minimise l'erreur du modèle sur l'ensembe des données d'entrainement. On va donc chercher à aller en direction opposée à celle du gradient du risque empirique. \n",
    "Plus l'erreur est grande et plus le \"pas de déplacement\" sera grand; nous nous déplacons toujours dans la direction opposée au gradient et proportionnellement à la magnitude de l'erreur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Régression linéaire régularisée (\"ridge regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Pour la ridge régression, l'expression mathématique donnant le risque empirique non régularisé (_risque empirique non régularisé + terme de régularisation_) est :\n",
    "$\\hat{R}_{\\lambda}(f_{\\theta}(x), D_{n}) = \\big(\\sum_{i=1}^n L(f_{\\theta}(x)^{(i)}, t^{(i)})\\big) + \\lambda \\Omega (\\theta) = \\sum_{i=1}^n L(f_{\\theta}(x)^{(i)}, t^{(i)}) + \\lambda\\|w\\|^{2}$\n",
    "\n",
    "Il est souvent nécessaire d'introduire une \"préférence\" pour certaines valeurs de paramètres plutôt que d'autres afin d'éviter le surapprentissage (_overfitting_). $\\Omega$ pénalise plus ou moins les différentes valeurs de paramètres. $\\lambda \\geq 0$ donne l'importance du terme de régularisation en relation avec le risque empirique. \n",
    "\n",
    "Ainsi la Ridge regression est défini comme étant égale à : _linear regression + quadratic (L2) regularization_. \n",
    "\n",
    "Le principe de minimisation du risque empirique régularisé s'exprime mathématiquement ainsi : \n",
    "\n",
    "$\\begin{equation*}\\{w^{*}, b^{*}\\} = \\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\hat{R_\\lambda}(f_\\theta, D_{n}) = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n L(f_{\\theta}(x)^{(i)}, t^{(i)})  + \\lambda\\|w\\|^{2} = \\underset{w, b}{\\operatorname{arg min}} \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})^2  + \\lambda\\|w\\|^{2} \\end{equation*}$\n",
    "\n",
    "où $\\|w\\|^{2} = \\sum_{j=1}^d w_{j}^{2}$\n",
    "\n",
    "L'optimisation de l'expression susdite par le gradient du risque empirique régularisé :\n",
    "\n",
    "$ \\nabla \\hat{R_{\\lambda}}(f_\\theta, D_{n}) = \\begin{equation*}\\begin{bmatrix} \\frac{\\partial\\hat{R_{\\lambda}}(f_\\theta, D_{n})}{\\partial b} \\\\ \\frac{\\partial\\hat{R_{\\lambda}}(f_\\theta, D_{n})}{\\partial w_{1}} \\\\ \\vdots \\\\ \\frac{\\partial\\hat{R_{\\lambda}}(f_\\theta, D_{n})}{\\partial w_{d}} \\end{bmatrix} = \\begin{bmatrix} \n",
    "\\sum_{i=1}^n \\frac{\\partial}{\\partial b} L(f_{\\theta}(x)^{(i)}, t^{(i)}) + \\lambda \\frac{\\partial}{\\partial b} \\|w\\|^{2} \\\\ \\sum_{i=1}^n \\frac{\\partial}{\\partial w_{1}} L(f_{\\theta}(x)^{(i)}, t^{(i)}) + \\lambda \\frac{\\partial}{\\partial w_{1}} \\|w\\|^{2}   \\\\ \\vdots \\\\ \\sum_{i=1}^n \\frac{\\partial}{\\partial w_{d}} L(f_{\\theta}(x)^{(i)}, t^{(i)}) + \\lambda \\frac{\\partial}{\\partial w_{d}} \\|w\\|^{2}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\sum_{i=1}^n \\frac{\\partial}{\\partial b} (w^{T}x^{(i)} + b - t^{(i)})^{2} + \\lambda \\frac{\\partial}{\\partial b} \\|w\\|^{2} \\\\ \\sum_{i=1}^n \\frac{\\partial}{\\partial w_{1}} (w^{T}x^{(i)} + b - t^{(i)})^{2} + \\lambda \\frac{\\partial}{\\partial w_{1}} \\|w\\|^{2}   \\\\ \\vdots \\\\ \\sum_{i=1}^n \\frac{\\partial}{\\partial w_{d}} (w^{T}x^{(i)} + b - t^{(i)})^{2} + \\lambda \\frac{\\partial}{\\partial w_{d}} \\|w\\|^{2}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)}) \\\\ 2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})*x_{1}^{(i)} + 2*\\lambda*w_{1}  \\\\ \\vdots \\\\ 2 * \\sum_{i=1}^n (w^{T}x^{(i)} + b - t^{(i)})*x_{d}^{(i)} + 2*\\lambda*w_{d}  \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)}) \\\\ 2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)})*x_{1}^{(i)} + 2*\\lambda*w_{1}  \\\\ \\vdots \\\\ 2 * \\sum_{i=1}^n (f_{\\theta}(x^{(i)}) - t^{(i)})*x_{d}^{(i)} + 2*\\lambda*w_{d}  \n",
    "\\end{bmatrix}\\end{equation*} $\n",
    "\n",
    "2) Pseudocode source de l'algorithme d'entraînement qui cherchera\n",
    "les paramètres optimaux qui minimisent le risque empirique régularisé\n",
    "R par descente de gradient batch\n",
    "\n",
    "$X$ est une matrice $n\\times d$ et $y$ est un vecteur $n\\times 1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{def train}(\\boldsymbol{X}, \\boldsymbol{y}): \\\\\n",
    "& \\boldsymbol{X} = [\\boldsymbol{1}, \\boldsymbol{X}] \\\\\n",
    "& \\boldsymbol{\\theta} = random(size=len(\\boldsymbol{X}[:])) \\\\\n",
    "& \\text{while} \\ \\Vert \\boldsymbol{\\theta} \\Vert \\gt \\epsilon: \\\\\n",
    "    & \\quad \\boldsymbol{\\nabla} = 2 \\boldsymbol{X}^T \\! \\boldsymbol{\\cdot} \\boldsymbol{e} \\\\\n",
    "    & \\quad \\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\eta / n \\ (\\boldsymbol{\\nabla} + 2 \\lambda \\boldsymbol{\\theta}) \\\\\n",
    "    & \\quad \\boldsymbol{e} = (\\boldsymbol{X} \\boldsymbol{\\cdot} \\boldsymbol{\\theta}) - \\boldsymbol{y} \\\\\n",
    "& end\n",
    "\\end{align*}\n",
    "\n",
    "3) Le risque empirique régularisé s'exprime sous forme matricielle (en considérant que le biais b=0) comme suit : \n",
    "$\\begin{equation*}\\hat{R_\\lambda}(f_\\theta, D_{n}) = L(f_{\\theta}(X), t)  + \\lambda\\|w\\|^{2} \\\\ = \\|Xw - t\\|^2  + \\lambda\\|w\\|^{2} \\\\ = (Xw - t)^T(Xw - t)  + \\lambda w^{T}w \\\\ = (Xw)^{T}(Xw) - (Xw)^{T}t - t^{T}(Xw) + t^{T}t + \\lambda w^{T}w \\\\ = w^{T}X^{T}Xw - 2(Xw)^{T}t + t^{T}t + \\lambda w^{T}w \\\\ = w^{T}X^{T}Xw - 2w^{T}X^{T}t + t^{T}t + \\lambda w^{T}w\\end{equation*}$\n",
    "\n",
    "Le problème d'optimisation s'exprime ainsi :\n",
    "$\\begin{equation*}\\{w^{*}\\} = \\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\hat{R_\\lambda}(f_\\theta, D_{n}) = \\underset{w}{\\operatorname{arg min}} (w^{T}X^{T}Xw - 2w^{T}X^{T}t + t^{T}t + \\lambda w^{T}w)\\end{equation*}$\n",
    "\n",
    "Le gradient du risque empirique sous forme matricielle devient alors : \n",
    "$\\frac{\\partial \\hat{R_\\lambda}(f_\\theta, D_{n})}{\\partial w} = \\frac{\\partial}{\\partial w} (w^{T}X^{T}Xw - 2w^{T}X^{T}t + t^{T}t + \\lambda w^{T}w) = 2X^{T}Xw - 2X^{T}t + 2\\lambda w $\n",
    "\n",
    "où  $X \\in \\mathbb{R}^{n*d}, w \\in \\mathbb{R}^{d}$ et $t \\in \\mathbb{R}^{n}$. \n",
    "\n",
    "\n",
    "4) La solution analytique à ce problème de minimisation du risque empirique s'exprime sous forme de calcul matriciel (en considérant que le biais b=0) et en fixant le gradient selon w à 0 : $\\frac{\\partial \\hat{R_\\lambda}(f_\\theta, D_{n})}{\\partial w} = 0$\n",
    "\n",
    "$\\frac{\\partial \\hat{R_\\lambda}(f_\\theta, D_{n})}{\\partial w} = 2X^{T}Xw - 2X^{T}t + 2\\lambda w = 0$\n",
    "\n",
    "On fixe donc ce gradient à 0 : \n",
    "\n",
    "$\n",
    " 2X^{T}Xw - 2X^{T}t + 2\\lambda w = 0 $\n",
    "\n",
    "$ <=> 2(X^{T}X + \\lambda I)w = 2X^{T}t$ \n",
    "\n",
    "$ <=> (X^{T}X + \\lambda I)w = X^{T}t$ \n",
    "\n",
    "$ <=> w^{*} = (X^{T}X + \\lambda I)^{-1}X^{T}t$ \n",
    "\n",
    "où  $X \\in \\mathbb{R}^{n*d}, I \\in \\mathbb{R}^{d*d}, w \\in \\mathbb{R}^{d}$ et $t \\in \\mathbb{R}^{n}$. \n",
    "\n",
    "- Si $\\lambda = 0$ alors on retombe sur la version de regression linéaire simple (non régularisée) : $w^{*} = (X^{T}X)^{-1}X^{T}t$.  \n",
    "Et dans ce cas il est également possible que $(X^{T}X)^{-1}$ ne soit pas inversible, car dire que $(X^{T}X)^{-1}$ est inversible est équivalent à dire que le rang X est égale à $d$. Ainsi lorsque $(X^{T}X)^{-1}$ n'est pas inversible, cela signifie que : \n",
    " le rang de X est $< d$, autrement dit que certaine colonne de X sont une combinaison linéaire du reste des colonnes de X, i.e. qu'au moins une variable prédictive _(feature)_ est redondante (dans le sens où elle peut être écrite comme une combinaison d'autres _features_). Donc pour que la matrice soit inversible, il faut éviter les dépendances linéaires entre les colonnes de X (non _multicolinéarité_). Avoir $n < d$ crée un problème de dimensionnalité et le système d'équation va avoir une infinité de solution (le vecteur w ne pourra pas être déterminé de manière unique à partir du système d'équation). Lorsqu'on a trop de _features_, une des solutions est de supprimer des features (_features selection_) ou d'utiliser la _regularisation_. \n",
    "\n",
    "- Si $\\lambda \\neq 0$ alors la matrice $(X^{T}X + \\lambda I)^{-1}$ est toujours inversible car $\\lambda I$ rajoute des élements dans la diagonale et donc $X^{T}X$ est une _full rank_ matrice, or on sait qu'un matrice _full rank_ est toujours inversible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Régression avec un pré-traitement non-linéaire fixe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe un moyen très simple d'étendre la régression linéaire à une régression non-linéaire : \n",
    "- prétraitrer les entrées en ulisant un ensemble de fonction non linéaires qui restent fixent\n",
    "- transformer $x$ de manière non linéaire en un $\\tilde{x}$ de plus haute dimension, i.e. remplacer $x -> \\phi(x)$ où $\\phi(x)$ est une fonction non linéaire fixe. \n",
    "- mais par contre le modèle reste tout de même linéaire par rapport aux paramètres du modèle : $\\theta={w, b}$ \n",
    "- cette méthode s'appele également \"l'expansion de base\" (basis expansion). \n",
    "\n",
    "Par exemple si on est en 1D : $x \\in \\mathbb{R}$, on peut considérer la transformation polynomiale suivante : \n",
    "$\\tilde{x} = \\phi_{poly^{k}}(x) = \\begin{equation*}\\begin{bmatrix} x & x^{2} & \\cdots & x^{k} \\end{bmatrix} = \\begin{bmatrix} x \\\\ x^{2} \\\\ \\vdots \\\\ x^{k} \\end{bmatrix}= \\begin{bmatrix} \\phi_{1}(x) \\\\ \\phi_{2}(x) \\\\ \\vdots \\\\ \\phi_{k}(x) \\end{bmatrix}\\end{equation*}$ , où $\\phi_{i}(x) = x^{i}, \\forall i \\in \\{1, ..., k\\}$\n",
    "\n",
    "On peut alors \"entrainer\" un regresseur non pas sur les $(x^{(i)},t^{(i)})$ de l'ensemble des données d'entrainement d'origine mais plutôt sur un ensemble transformé, les $(\\phi(x^{i}),t^{(i)})$. Cette entrainement trouve les paramètres d'une fonction affine $f$. \n",
    "\n",
    "La prédiction pour un nouveau point test $x$ est alors obtenue non pas par $f(x)$ mais par $\\tilde{f}(x) = f(\\phi(x))$\n",
    "\n",
    "1) La forme paramétrique qu'on obtient pour $\\tilde{f}(x)$ (avec $x \\in \\mathbb{R}$) si on utilise $\\phi=\\phi_{poly^{k}}(x)$ : \n",
    "$\\tilde{f}(x) = w^{T}\\phi(x) + b = \\sum_{i=1}^k w_{i}\\phi_{i}(x) + b = w_{1}\\phi_{1}(x) + w_{2}\\phi_{2}(x) + ... + w_{k}\\phi_{k}(x) + b = w_{1}x + w_{2}x^{2} + ... + w_{k}x^{k} + b $ \n",
    "\n",
    "$\\hspace{3mm}$ où $w = \\begin{equation*}\\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{k} \\end{bmatrix}\\end{equation*}$ et $\\phi(x)=\\phi_{poly^{k}}(x) = \\begin{equation*}\\begin{bmatrix} \\phi_{1}(x) \\\\ \\phi_{2}(x) \\\\ \\vdots \\\\ \\phi_{k}(x) \\end{bmatrix} = \\begin{bmatrix} x \\\\ x^{2} \\\\ \\vdots \\\\ x^{k} \\end{bmatrix}\\end{equation*}$\n",
    "\n",
    "\n",
    "2) Les paramètres à estimer sont : $\\theta = \\{w \\in \\mathbb{R}^{k}, b \\in \\mathbb{R}\\}$\n",
    "\n",
    "3) Pour $\\phi_{poly^{1}}(x) = \\begin{equation*} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix}\\end{equation*}$ et donc on aura $\\tilde{f}(x) = \\sum_{i=1}^m w_{i}\\phi_{i}(x) + b = w_{1}x_{1} + w_{2}x_{2} + b$  $\\hspace{5mm}$  où $m=2$ est le nombre de paramètres de $w$ (nombre d'élements de $\\phi_{poly^{1}}(x)$) et donc en tout $m+1$ = $3$ paramètres au total (+1 pour le biais b)\n",
    "\n",
    "Pour $\\phi_{poly^{2}}(x) = \\begin{equation*} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{1}^{2} \\\\ x_{2}^{2} \\\\ x_{1}x_{2} \\end{bmatrix}\\end{equation*}$  et donc on aura $\\tilde{f}(x) = \\sum_{i=1}^m w_{i}\\phi_{i}(x) + b = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{1}^{2} + w_{4}x_{2}^{2} + w_{5}x_{1}x_{2} + b$ $\\hspace{5mm}$  où $m=5$ est le nombre de paramètres de $w$ (nombre d'élements de $\\phi_{poly^{2}}(x)$) et donc en tout $m+1$ = $6$ paramètres au total (+1 pour le biais b) \n",
    "\n",
    "Pour $\\phi_{poly^{3}}(x) = \\begin{equation*} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{1}^{2} \\\\ x_{2}^{2} \\\\ x_{1}^{3} \\\\ x_{2}^{3} \\\\ x_{1}x_{2} \\\\ x_{1}x_{2}^{2} \\\\ x_{1}^{2}x_{2} \\end{bmatrix}\\end{equation*}$  et donc on aura $\\tilde{f}(x) = \\sum_{i=1}^m w_{i}\\phi_{i}(x) + b = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{1}^{2} + w_{4}x_{2}^{2} + w_{5}x_{1}^{3} + w_{6}x_{2}^{3} + w_{7}x_{1}x_{2} + w_{8}x_{1}x_{2}^{2} + w_{9}x_{1}^{2}x_{2} + b$ $\\hspace{5mm}$  où $m=9$ est le nombre de paramètres de $w$ (nombre d'élements de $\\phi_{poly^{ 3}}(x)$) et donc en tout $m+1$ = $10$ paramètres au total  (+1 pour le biais b)\n",
    "\n",
    "4) __Quelle sera la dimensionalité de $\\phi_{poly^{k}}(x)$, en fonction de $d$ et $k$ ? __ \n",
    "\n",
    "Pour chacune des puissances de $1,...,k$ on a besoin de $d$ paramètres donc $kd$; et à cela on doit prendre également en compte les combinaisons de _cross terms_ (dont la puissance est $\\leq k$). Ainsi si on fixe $p$ comme étant le degré des polynomes de $1$ jusqu'à $k$ alors la dimension totale de $\\phi_{poly^{k}}(x)$ en fonction de $d$ et $k$ sera égale à : \n",
    "$ \\sum_{p=1}^k {d+p-1\\choose p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
