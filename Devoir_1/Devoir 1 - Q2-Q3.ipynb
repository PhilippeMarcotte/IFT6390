{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Fléau de la dimensionalité et intuition géométrique en haute dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calcul du volume d'un hypercube (d-dimensions) de côté c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le volume d'un hypercube (de côté c), appelé aussi hypervolume est défini comme étant l'intégrale sur la région de définition de l'hypercube (c'est-à-dire $d$-intégrales entre [0,c], soit donc une intégrale sur le domaine de définition de chacune des $d$-dimensions). \n",
    "D'un point de vue mathématiques, on a :\n",
    "\n",
    " $$\\int_0^c \\int_0^c ... \\int_0^c 1 \\,dx_1\\,dx_2...\\,dx_d = c^d$$\n",
    " \n",
    "Le volume d'un hypercube (de dimension $d$, en unité $cm^d$) est donc égale à : <font color=blue> $Volume(hypercube) = c^d$ </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Densité de probabilité à l'intérieur de l'hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On appelle densité de probabilité d'une variable aléatoire $X$ à valeur dans $R^d$ une fonction (mathématiques) $f$ telle que pour toute Tribu borélienne : \n",
    "\n",
    "$$\\mathbb{p}(x \\in Region) : \\int_{Region}\\ f_X(x)\\,dx = 1$$ \n",
    "\n",
    "$$\\mathbb{p}(x) : \\int_0^c \\int_0^c ... \\int_0^c f_X(x) \\,dx_1\\,dx_2...\\,dx_d = 1$$\n",
    "$$f_X(x) \\int_0^c \\int_0^c ... \\int_0^c \\,dx_1\\,dx_2...\\,dx_d = 1$$\n",
    "$$f_X(x) * c^d = 1$$\n",
    "<font color=blue> $$f_X(x) = \\frac{1}{c^d}$$ </font>\n",
    "\n",
    "\n",
    "Comme mentionné dans la formule ci-dessus, cela revient donc à faire que : le produit entre ma $pdf$ et l'hypervolume de l'hypercube soit égale à 1 (car la $pdf$ doit toujours être positive et son intégrale doit être égale à 1 $cf$. propriétés des $pdf$). Donc finalement la fonction de densité de probabilité (p(x)) pour l'hypercube est définie de manière à ce qu'elle vaut <font size=4 color=blue> $\\frac{1}{c^d}$ </font>  si $x$ est à l'intérieur du cube, et 0 si $x$ est à l'extérieur du cube.\n",
    "\n",
    "Les propriétés des \"probability density functions\" utilisées pour répondre à cette question sont : \n",
    "$$\\int_{Region}\\ f_X(x)\\,dx = 1$$ \n",
    "\n",
    "$$f_X(x) \\geq 0$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Densité de probabilité à l'intérieur du petit hypercube et zone de bordure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note notre nouvelle longueur de côté de l'hypercube intérieur comme suit : $k$ = $c$ - $\\frac{2 * 3 * c}{100}$. \n",
    "\n",
    "On commence d'abord par calculer la probabilité que le point tombe dans le petit-hypercube intérieur :\n",
    "$$ \\int_0^k \\int_0^k ... \\int_0^k \\mathbb{p}(x) \\,dx_1\\,dx_2...\\,dx_d = \\mathbb{p}(x) \\int_0^k \\int_0^k ... \\int_0^k \\,dx_1\\,dx_2...\\,dx_d = \\frac{1}{c^d} * k^d $$\n",
    "\n",
    "Où $k^d$ est l'hypervolume du __petit-hypercube__ et <font size=4 color=blue> $\\frac{k^d}{c^d}$ </font> est donc __la probabilité que le point tombe dans ce petit-hypercube__.\n",
    "\n",
    "On peut désormais, calculer la probabilité que le point tombe dans __la zone de bordure__ comme suit : \n",
    "$$\\mathbb{p}(x \\in etroite\\_bordure) = 1-probabilité\\_de\\_tomber\\_dans\\_le\\_petit\\_hypercube = 1 - \\frac{k^d}{c^d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculs numériques de probabilités sur l'étroite bordure selon différentes dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{Rappel}$ : soit $c$ = 100cm (côté du grand hypercube), la zone de bordure étroite est d'une largeur de 3% de $c$ bordant l'hypercube vers l'intérieur, et donc le petit hypercube intérieur est de côté $k$ = $c$ - $\\frac{2 * 3 * c}{100}$ = $100$ - $\\frac{2 * 3 * 100}{100} = 94cm.$\n",
    "\n",
    "* la probabilité pour que $x$ tombe dans le petit hypercube intérieur de dimension d est égale à : $$\\frac{k^d}{c^d} = \\frac{94^d}{100^d}$$\n",
    "* la probabilité pour que $x$ tombe dans l'étroite bordure (zone de bordure) de dimension d est égale à : $$1 - \\frac{k^d}{c^d} = 1 - \\frac{94^d}{100^d}$$\n",
    "\n",
    "+ En dimension d = 1 : $$1 - \\frac{k}{c} = 1 - \\frac{94}{100} = 0.06$$\n",
    "+ En dimension d = 2 : $$1 - \\frac{k^2}{c^2} = 1 - \\frac{94^2}{100^2}=0.116$$\n",
    "+ En dimension d = 3 : $$1 - \\frac{k^3}{c^3} = 1 - \\frac{94^3}{100^3} = 0.169$$\n",
    "+ En dimension d = 5 : $$1 - \\frac{k^5}{c^5} = 1 - \\frac{94^5}{100^5} = 0.266$$\n",
    "+ En dimension d = 10 : $$1 - \\frac{k^{10}}{c^{10}} = 1 - \\frac{94^{10}}{100^{10}} = 0.461$$\n",
    "+ En dimension d = 100 : $$1 - \\frac{k^{100}}{c^{100}} = 1 - \\frac{94^{100}}{100^{100}} = 0.998$$\n",
    "+ En dimension d = 1000 : $$1 - \\frac{k^{1000}}{c^{1000}} = 1 - \\frac{94^{1000}}{100^{1000}} = 1 - \\frac{\\infty}{\\infty}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Que conclure sur la répartition des points en haute dimension qui est contraire à notre intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes bien dans ce qu'on appele le ___\"fléau de la dimensionnalité\"___, i.e. que plus on augmente le nombre de dimensions et plus les points d'une distribution semble se rapprocher des limites de l'espace (de la fine bordure de l'hypercube). Ainsi la probabilité qu'un point tombe dans l'hypercube intérieur est extremement petit; et plus $d\\to\\infty$ et plus la probabilité qu'un point $x$ tombe dans l'étroite bordure devient forte. Cela est illustrée par l'exemple ci-dessus, on voit bien que lorsque le nombre de dimensions $d$ tend vers l'infini alors la probabilité que les points se concentre sur les bordures de l'hypercube tend vers 1.\n",
    "\n",
    "Bien que nous puissions utiliser notre intuition à deux et trois dimensions pour comprendre certains aspects de la géométrie, il existe néanmoins de nombreuses contre-intuitions lorsque nous sommes en haute-dimension (une vaste majorité de la région est vide).\n",
    "\n",
    "Par ailleurs, il est à noter que cette ___\"malédiction de la dimensionnalité\"___ affecte également la capacité à bien généraliser : la difficulté à bien généraliser peut donc potentiellement augmenter exponentiellement avec la dimension \"d\" des entrées. A chaque fois que l'on rajoute une dimension à notre hypercube, on devra peupler ces nouvelles régions induite par la nouvelle dimension; cela nécessite donc de plus en plus de data (croissance exponentielle) pour couvrir toutes les régions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercice 3 : Estimation de densité paramétrique Gaussienne, vs estimation de densité par fenêtres de Parzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimation de densité paramétrique avec une densité Gaussienne isotropique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel une estimation de densité consiste, étant donné un Dataset $D = \\{x^{(1)},...,x^{(n)}\\}$ composé de $n$ exemples et avec $x \\in R^{d}$, à estimer la fonction de densité $p(x)$ ayant pu générer ces données de telle manière que pour chaque nouveau point test $x$ nous pourrons dans le futur calculer $p(x)$. Par ailleurs, il est à noter également que dans le cas d'une Gaussienne isotropique (\"sphérique\") en $d$-dimensions, la colline de la Gaussienne est centrée en $\\mu$ avec une largeur (écart-type) $\\sigma$ qui est la même dans toutes les directions. _Ici je suppose avoir dérivé le Maximum Likelihood pour mon estimateur $\\hat{\\sigma^{2}}$ au préalable (sans y faire apparaitre de démo exhaustive)._ \n",
    "\n",
    "a) Les deux paramètres à estimer lors de l'apprentissage sont :\n",
    "- $\\mu$ est un vecteur en dimension $d$ tel que : $\\mu = (\\mu_1,...,\\mu_d)$ qui contient pour chaque $\\mu_j$ la moyenne pondérée de la dimension $x_j$ (et donc de chaque *features*). Concretement, $\\mu$ indique où se trouve le centre (i.e. la moyenne) de ma Gaussienne multivariée (de ma colline) dans mon espace vectoriel (continue) de dimension $d$. Plus $x$ sera proche de $\\mu$ et plus sa densité de probabilité sera élevée. \n",
    "- et la variance $\\sigma^{2}$ qui est un scalaire car nous considérons une *__Gaussienne isotropique__* et donc la variance est la même pour chacune des dimensions (*features*). On le paramètre $\\sigma^{2}$ est un scalaire ce qui signifie que la forme de la covariance. C'est plus simple (pour nous et pour l'ordinateur) à calculer, mais ça signifie aussi que notre modèle est moins puissant.\n",
    "\n",
    "b) Si on apprend les paramètres de cette Gaussienne isotropique en utilisant le principe de maximum de vraisemblance (ça se fait très facilement en calculant les dérivées partielles et en les mettant à 0), la formule qui nous donnera la valeur des paramètres optimaux est : \n",
    "- pour chaque composante $\\hat{\\mu_j}$ de $\\hat{\\mu}$ on aura : $\\hat{\\mu_j} = \\frac{1}{n}\\sum_{i=1}^n x_j^{(i)}$, sous forme vectorielle (d-dimensions) cela donne : $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}$\n",
    "- pour la variance (qui rappelons le est un scalaire) on aura : $\\hat{\\sigma^{2}} = \\frac{1}{n*d}\\sum_{i=1}^n (x^{(i)} - \\hat{\\mu})^{T}*(x^{(i)} - \\hat{\\mu})$ \n",
    "Ceci ce démontre analytiquement comme à la question 3.4.d très facilement avec le MLE et dérivées.\n",
    "<br>Une autre manière pour calculer la variance aurait pu être de passer par la matrice de covariance $\\sum = \\frac{1}{n}\\sum_{i=1}^n (x^{(i)} - \\mu)*(x^{(i)} - \\mu)^{T}$ en sachant que tous les $\\sigma^{2}$ en diagonale sont identiques (car Gaussienne isotropique) et que $\\sum = \\sigma^2*I$\n",
    "\n",
    "c) Dans le présent cas, la phase de *training* consiste à apprendre les paramètres $\\mu$ et $\\sigma^{2}$, et donc en terme de complexité algorithmique du calcul de chacun de ces paramètres on aura : \n",
    "- complexité de calcul pour $\\mu$        :  $O(nd)$\n",
    "- complexité de calcul pour $\\sigma^{2}$ :  $O(nd)$\n",
    "\n",
    "d) Pour un nouveau point test $x$, la fonction qui donnera la densité de probabilité prédite au point $x$ est : \n",
    "- $\\hat{p}_{gauss-isotrop}(x) = \\mathcal{N}_{\\mu, \\sigma^{2}}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-\\mu\\|^{2}}{\\sigma^{2}}}$\n",
    "\n",
    "e) La complexité de cette prédiction à chaque nouveau point $x$ est : $O(d)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimation de densité en utilisant des fenêtres de Parzen avec un noyau Gaussien isotropique de largeur (écart-type) $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes à présent dans le cas où nous essayons d'estimé la densité de probabilité en utilisant des fenêtres de Parzen avec un noyau Gaussien istropique (de largeur, écart-type $\\sigma$) et qu'on a entraîné ces fenêtres de Parzen sur $D$. \n",
    "\n",
    "a) Dans le présent cas, la phase \"entrainement/apprentissage\" consiste juste à mémoriser les données d'entrainement (il n'y a pas de paramètres à apprendre). On fixe l'hyperparamètre $\\sigma$ et on mémorise les données. \n",
    "\n",
    "b) Pour cet algorithme de Parzen à noyau Gaussien isotropique, La fonction qui donnera la densité de probabilité prédite au point $x$ est définie comme suit : \n",
    "- $\\hat{p}_{Parzen}(x) = \\frac{1}{n}\\sum_{i=1}^n K(X_{i};x)$ où $K(X_{i};x) = \\mathcal{N}_{X_{i}, \\sigma^{2}}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-X_{i}\\|^{2}}{\\sigma^{2}}}$ est un noyau gaussien isotropique centré en $X_{i}$. \n",
    "___Donc l'estimateur de densité de Parzen se re-écrit comme suit : $\\hat{p}_{Parzen}(x) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-X_{i}\\|^{2}}{\\sigma^{2}}}$ ___\n",
    "\n",
    "c) La complexité de cette prédiction à chaque nouveau point $x$ est : $O(nd)$\n",
    "\n",
    "Nous avons besoin de traiter et de parcours tous les points d'entrainement pour faire la prédiction (car on calcul pour chaque point $X_{i}$ l'application du kernel $K(X_{i};x))$ sur le _test point_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Capacité/Expressivité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "a) Avec peu de données, les modèles avec densité paramétrique Gaussienne peuvent bien fonctionner, alors que les estimations avec noyaux nécessitent beaucoup plus de données. L'approche de Parzen est plus souple et plus riche que l'approche paramétrique Gaussienne. Par ailleurs, Parzen KDE est plus expressif que la densité paramétrique Gaussienne car il a également plus d'hyperparamètres qui sont : la _bandwith_ $\\sigma$ (la largeur du noyau est un paramètre libre qui influence fortement l'estimation obtenue) et le kernel K Gaussien isotropique et ses paramètres; Parzen KDE possède également un degré de liberté plus elevé que la Gaussienne paramétrique. Etant donné que $\\sigma$ est considéré comme un hyperparamètre, on peut _tuner_ ce dernier pour avoir différents résultats (et ainsi on peut optimiser le $\\sigma$). On peut également noter que l'inférence (prédiction) est beaucoup plus rapide avec la _Gaussienne Paramétrique_ qu'avec la méthode avec _Parzen_ qui elle requiert de parcourir tout le _trainset_.\n",
    "\n",
    "b) Dans l'approche avec Parzen à noyau Gaussien, on a un hyperparamètre $\\sigma$ qui est fixé à l'avance et donc si ce dernier ($\\sigma$) est fixé trop élevé par rapport aux données de _training_ alors on aura de grande chance d'être en _sur-apprentissage._ \n",
    "\n",
    "c) Le $\\sigma$ dans les fenêtres de Parzen est traité comme un hyperparamètre, c'est-à-dire qu'il est fixé à l'avance avant l'entrainement car il sera optimisé et ajusté qu'au moyen des ensembes de validation. Et également parce que dans l'estimation de Parzen avec noyau, le $\\sigma$ défini un modèle qui reste inchangé durant l'apprentissage. Le choix du bon hyperparamètre $\\sigma$ sera effectué par un essai de plusieurs valeurs, en déterminant visuellement ou quantitativement la qualité. \n",
    "\n",
    "En revanche, dans le cas de l'estimation de densité paramétrique Gaussienne le $\\sigma$ est considéré comme un paramètre puisqu'il est appris et optimisé à l'apprentissage sur les données d'entrainement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Estimation de densité paramétrique avec une densité Gaussienne diagonale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) L'équation d'une densité Gaussienne diagonale dans $R^{d}$ est définie comme suit : \n",
    "$p_{gauss-diagonal}(x) = \\mathcal{N}_{\\mu, \\sum}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sqrt{\\mid\\sum\\mid}} e^{(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))}$\n",
    "\n",
    "Les deux paramètres à estimer lors de l'apprentissage sont :\n",
    "- $\\mu$ (moyenne empirique) est un vecteur colonne en dimension $d$ tel que : $\\mu = (\\mu_1,...,\\mu_d)$ qui contient pour chaque $\\mu_j$ la moyenne pondérée de la dimension $x_j$ (et donc de chaque *features*). Concretement, $\\mu$ indique où se trouve le centre (i.e. la moyenne) de ma Gaussienne multivariée (de ma colline) dans mon espace vectoriel (continue) de dimension $d$. Plus $x$ sera proche de $\\mu$ et plus sa densité de probabilité sera élevée. \n",
    "- et $\\sum$ (covariance empirique) est la matrice de covariance diagonale de dimension $d*d$ definie telle que : \n",
    "\n",
    "$\\sum = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$\n",
    "\n",
    "$\n",
    "\\sum_{ij} = \\begin{cases}\n",
    "\\sigma_{i}^{2}&\\text{si $i=j$}\\\\\n",
    "0&\\text{sinon}\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Le second paramètre à apprendre sont les variances $\\sigma^{2} \\in R^{d}$ (vecteur de dimension $d$) qui forment les élements diagonaux de la matrice de covariance $\\sum \\in R^{d*d}$, et ce $\\sigma^{2}$ peut être nôté sous forme de vecteur de dimension $d$ tel que $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$ et que toutes les variances $\\sigma_{i}^{2}$ en diagonale de la matrice de covariance sont non-nuls alors que tous les autres élèments (où $i\\neq j$) sont $\\sigma_{i, j}^{2} = 0$\n",
    "\n",
    "Ainsi on peut apprendre les composantes du second paramètre à apprendre qui est le vecteur de $d$-dimensions $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$ comme suit : \n",
    "$\\hat{\\sigma}_{i}^{2} = \\hat{\\sum_{ii}} = \\frac{1}{n}\\sum_{j=1}^n (x_{i}^{(j)} - \\hat{\\mu_{i}})*(x_{i}^{(j)} - \\hat{\\mu_{i}})$\n",
    "\n",
    "b) Dans le présent cas, on souhaite montrer que dans le cas où un vecteur aléatoire suit une distribution Gaussienne diagonale (dont les composantes du vecteur suivent une _jointly Gaussian_) alors les composantes de ce vecteur aléatoire sont des variables aléatoires indépendendantes. On peut prouver cela de plusieurs manières, mais ici nous prendrons également l'exemple à deux variables distribuées selon une Gausienne diagonale. \n",
    "\n",
    "Exemple en 2-dimension : soit le vecteur _aléatoire_ x,  le vecteur _mean_ $\\mu$ et la matrice de _covariance_ $\\Sigma$\n",
    "\n",
    "$ x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} $ \n",
    "$ \\mu = \\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{bmatrix} $ \n",
    "$ \\Sigma = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22}\\end{bmatrix} $ \n",
    "\n",
    "Montrons que si les composantes du vecteur aléatoire sont _uncorrelated_ et qu'ils suivent une _jointly Gaussian_ alors cela implique que les composantes du vecteur aléatoire sont _independants_ : \n",
    "En factorisant la _pdf (fonction de densité de probabilité)_ de la gaussienne multivariée, on qu'étant donnée que $x_{1}$ et $x_{2}$ sont non-correlés alors cela signifie que $\\sigma_{i, j}^{2} = 0$ pour $i\\neq j$ et donc que $(x^{(i)}-\\mu)^{T} \\Sigma^{-1} (x^{(i)}-\\mu)$ a la forme partitionnée suivante : \n",
    "$\\begin{bmatrix} x_{1}-\\mu_{1} \\\\ x_{2}-\\mu_{2} \\end{bmatrix}^{T}\n",
    "\\begin{bmatrix} \\Sigma_{11} & 0 \\\\ 0 & \\Sigma_{22} \\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix} x_{1}-\\mu_{1} \\\\ x_{2}-\\mu_{2} \\end{bmatrix}$\n",
    "\n",
    "$ = \\begin{bmatrix} x_{1}-\\mu_{1} \\\\ x_{2}-\\mu_{2} \\end{bmatrix}^{T}\n",
    "\\begin{bmatrix} \\frac{1}{\\sigma_{1}^{2}} & 0 \\\\ 0 & \\frac{1}{\\sigma_{2}^{2}} \\end{bmatrix}\n",
    "\\begin{bmatrix} x_{1}-\\mu_{1} \\\\ x_{2}-\\mu_{2} \\end{bmatrix}$\n",
    "\n",
    "$ = \\frac{(x_{1}-\\mu_{1})^{2}}{\\sigma_{1}^{2}} + \\frac{(x_{2}-\\mu_{2})^{2}}{\\sigma_{2}^{2}}$\n",
    "\n",
    "Que l'on peut noter cela d'un point de vue informel : \n",
    "$ = (x_{1}-\\mu_{1})^{T} \\Sigma_{11}^{-1} (x_{1}-\\mu_{1}) + (x_{2}-\\mu_{2})^{T} \\Sigma_{22}^{-1} (x_{2}-\\mu_{2})$. \n",
    "\n",
    "Donc nous voyons bien que nous pouvons factoriser l'exponentiel $e^{(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))}$ en produit de deux exponentiels : $e^{(-\\frac{1}{2}(x_{1}-\\mu_{1})^{T} \\Sigma_{11}^{-1} (x_{1}-\\mu_{1}))}$ et $e^{(-\\frac{1}{2}(x_{2}-\\mu_{2})^{T} \\Sigma_{22}^{-1} (x_{2}-\\mu_{2}))}$ : \n",
    "Par conséquent la _pdf_ (fonction de densité de probabilité) peut être reécrit comme : $P(x;\\mu, \\Sigma) = P(x_{1}, x_{2};\\mu, \\Sigma) = P(x_{1};\\mu_{1}, \\sigma_{1}) \\cdot P(x_{2};\\mu_{2}, \\sigma_{2})$ provant ainsi que $x_{1}$ et $x_{2}$ sont indépendents. \n",
    "\n",
    "Ceci étant dit, on peut généraliser cette méthode à toutes les variables aléatoires $X_{1}...X_{d}$ d'un vecteur aléatoire de dimension $d$ distribuée selon une Gaussienne diagonale multivariée. \n",
    "\n",
    "_preuve :_ Si $X_{1}...X_{d}$ sont des variables aléatoires non-corrélées (i.e. que $\\sigma_{i, j}^{2} = 0$ pour $i\\neq j$) alors $\\Sigma$ et $\\Sigma^{-1}$ sont des matrices diagonales où $\\sigma_{i, i} = \\Sigma_{i, i} = \\sigma_{i} = Var[X_{i}]$, \n",
    "\n",
    "$|\\Sigma| = \\prod_{i=1}^{d} \\Sigma_{i, i} = \\prod_{i=1}^{d} \\sigma_{i}$\n",
    "\n",
    "$ \\Sigma^{-1} = \\begin{bmatrix} \\frac{1}{\\sigma_{1}} & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\frac{1}{\\sigma_{d}} \\end{bmatrix}$\n",
    "\n",
    "Cela signifie que $(x-\\mu)^{T} \\Sigma^{-1} (x-\\mu) = \\sum_{i=1}^{d}\\frac{(x_{i}-\\mu_{i})^{2}}{\\sigma_{i}}$ et la densité est : \n",
    "$\\prod_{i=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma_{ii}}} e^{\\big(-\\frac{1}{2}\\frac{(x_{i}-\\mu_{i})^{2}}{\\sigma_{ii}}\\big)}$\n",
    "\n",
    "Par conséquent, comme la fonction de densité jointe pour $X_{1}...X_{d}$ est un produit des densités, cela signifie bien que les variables sont indepédentes. \n",
    "\n",
    "_Donc les variables aléatoires d'un vecteur aléatoire distribué selon une Gaussienne diagonale multivariée sont independantes._\n",
    "\n",
    "\n",
    "\n",
    "c) Rappel sur le Maximum de vraissemblance : \n",
    "L'estimation par maximum de vraisemblance est une technique statistique qui permet la construction de l'estimateur du maximum de vraisemblance (_Maximum Likelihood Estimator_). On part du principe que la variable d'intérêt suit une certaine loi de probabilité (içi une _Gaussienne multivariée diagonale_) et on cherche à estimer les paramètres inconnus ($\\theta$ : ici $\\mu$ et $\\sum$) caractérisant cette loi. Pour cela on se sert de la vraisemblance qui sert à mesurer la probabilité que des observations (_data_) proviennent d'une loi donnée; l'estimateur du maximum de vraisemblance est déterminé en maximisant cette vraisemblance (ou en minimisant le risque empirique sur l'ensemble du _training set_ via $-\\log(p(x \\mid \\theta))$).   \n",
    "\n",
    "$\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\hat{R}(P_\\theta, D_{n}) = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n (-\\log(p(x^{(i)}\\mid\\theta)))$\n",
    "\n",
    "$\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n (-\\log(\\frac{1}{(2\\pi)^{d/2} \\mid\\sum\\mid^{1/2}} e^{-\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu)})) $\n",
    "\n",
    "$\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\sigma^{2}_1 ...\\sigma^{2}_d) + \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{j}^{(i)}-\\mu_{j})^{2}}{\\sigma_{j}^{2}} + const)$\n",
    "\n",
    "Cela est également égal à : $\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu) + const)$\n",
    "\n",
    "On peut se débarasser de la constante _const_ car elle varie pas avec les paramètres et donc elle n'aura aucun effet la solution. \n",
    "\n",
    "On note $J(\\mu, \\sum) = \\sum_{i=1}^n (\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu))$ où $\\theta = \\mu, \\sum$ sont les paramètres.\n",
    "\n",
    "Donc de manière simplifiée l'équation ci-dessus s'écrira : $\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} J(\\mu, \\sum) $\n",
    "\n",
    "Avec $\\theta^{*}$ qui correspond aux paramètres estimés : $\\hat{\\mu}, \\hat{\\sum}$ et $\\theta$ correspond aux paramètres : $\\mu, \\sum$\n",
    "\n",
    "d) __Résolution analytique de l'équation ci-dessous se fait en utilisant les dérivées partielles afin de trouver les paramtères optimaux $\\theta^{*}: \\hat{\\mu}, \\hat{\\sum}$__\n",
    "\n",
    "Fixons comme précédement : $J(\\mu, \\sum) = \\sum_{i=1}^n (\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu))$ alors on a l'équation suivante : $\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} J(\\theta)$  et se reécrit également : $\\hat{\\mu}, \\hat{\\sum} = \\underset{\\mu, \\sum}{\\operatorname{arg min}} J(\\mu, \\sum) $ \n",
    "$\\hat{\\mu}, \\hat{\\sum} = \\underset{\\mu, \\sum}{\\operatorname{arg min}} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu))$\n",
    "\n",
    "On aurait pu également fixer comme risque empirique à minimiser : $J(\\mu, \\sigma^{2}_1,...,\\sigma^{2}_d) = \\sum_{i=1}^n (\\frac{1}{2} \\log(\\sigma^{2}_1 ...\\sigma^{2}_d) + \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{j}^{(i)}-\\mu_{j})^{2}}{\\sigma_{j}^{2}} + const)$\n",
    "\n",
    "Et donc dans ce cas on aurait eu (c'est strictement équivalent à la fonction de coût susdite, car la matrice de covariance est diagonale) : \n",
    "$\\hat{\\mu}, \\hat{\\sum} = \\underset{\\mu, \\sum}{\\operatorname{arg min}} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\sigma^{2}_1 ...\\sigma^{2}_d) + \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{j}^{(i)}-\\mu_{j})^{2}}{\\sigma_{j}^{2}} + const)$\n",
    "\n",
    "- La moyenne empirique $\\hat{\\mu}$ est trouvée lorsque : $\\frac{\\partial J}{\\partial \\mu} = 0$\n",
    "\n",
    "$\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial \\mu} & = \\frac{\\partial}{\\partial \\mu} \\sum_{i=1}^n \\Bigg(\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\mu)^{T}\\Sigma^{-1}(x^{(i)}-\\mu)\\Bigg)  \\\\\n",
    " & = \\frac{\\partial}{\\partial \\mu} \\sum_{i=1}^n \\Bigg(\\frac{1}{2} \\mu^{T}\\Sigma^{-1}\\mu - \\mu^{T}\\Sigma^{-1}x^{(i)}\\Bigg)\\\\ \n",
    " & = \\Sigma^{-1} \\sum_{i=1}^n (\\mu - x^{(i)}) = 0   \\to \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Donc pour résumé, pour chaque composante $\\mu_j$ de $\\mu$ on aura : $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_j^{(i)}$, sous forme vectorielle ($d$-dimensions) cela donne : $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}$ \n",
    "\n",
    "- La matrice de covariance empirique $\\hat{\\sum}$ est trouvée lorsque : $\\frac{\\partial J(\\mu, \\sum)}{\\partial \\sum} = 0$ \n",
    "\n",
    "$\n",
    "\\begin{equation} \\label{eq2}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J(\\hat{\\mu}, \\sum)}{\\partial \\sum} & = \\frac{\\partial}{\\partial \\sum} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\mid\\sum\\mid) +\\frac{1}{2}(x^{(i)}-\\hat{\\mu})^{T}\\Sigma^{-1}(x^{(i)}-\\hat{\\mu}))  \\\\\n",
    " & =  \\frac{1}{2} \\sum_{i=1}^n (-\\Sigma^{-1}(x^{(i)}-\\hat{\\mu})(x^{(i)}-\\hat{\\mu})^{T}\\Sigma^{-1} + \\Sigma^{-1})\\\\ \n",
    " & =  \\frac{1}{2}\\Sigma^{-1}\\Bigg[-\\Big(\\sum_{i=1}^n(x^{(i)}-\\hat{\\mu})(x^{(i)}-\\hat{\\mu})^{T}\\Big)\\Sigma^{-1} + n*I\\Bigg] = 0   \\to \\hat{\\sum} = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)}-\\hat{\\mu})(x^{(i)}-\\hat{\\mu})^{T}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "La matrice de covariance estimée par le MLE (Maximum Likelihood) se calcul comme suit : \n",
    "$\\hat{\\sum} = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$\n",
    "Et on peut acceder à chacune des variances $\\sigma_{i}^{2}$ de $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$ contenues dans la diagonale de la matrice de covariance $\\hat{\\sum}$ comme cela :  $\\hat{\\sigma}_{i}^{2} = \\hat{\\sum_{ii}} = \\frac{1}{n}\\sum_{j=1}^n (x_{i}^{(j)} - \\hat{\\mu_{i}})*(x_{i}^{(j)} - \\hat{\\mu_{i}})$\n",
    "\n",
    "- Cette estimation de la diagonale $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$ de la matrice diagonale peut être également obtenue analytiquement en fixant : $J(\\mu, \\sigma^{2}_1,...,\\sigma^{2}_d) = \\sum_{i=1}^n (\\frac{1}{2} \\log(\\sigma^{2}_1 ...\\sigma^{2}_d) + \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{j}^{(i)}-\\mu_{j})^{2}}{\\sigma_{j}^{2}} + const)$\n",
    "Et le vecteur de variance en diagonale $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$ sera trouvé en posant : \n",
    "$\\hat{{\\sigma^{2}_i}}$ est trouvée lorsque : $\\frac{\\partial J(\\hat{\\mu}, \\sigma^{2}_1,...,\\sigma^{2}_d)}{\\partial \\sigma^{2}_k} = 0$\n",
    "\n",
    "$\n",
    "\\begin{equation} \\label{eq3}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J(\\hat{\\mu}, \\sigma^{2}_1,...,\\sigma^{2}_d)}{\\partial \\sigma^{2}_k} & = \\frac{\\partial}{\\partial \\sigma^{2}_k} \\sum_{i=1}^n (\\frac{1}{2} \\log(\\sigma^{2}_1 ... \\sigma^{2}_k ... \\sigma^{2}_d) + \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{j}^{(i)}-\\mu_{j})^{2}}{\\sigma_{j}^{2}})  \\\\\n",
    " & =  \\sum_{i=1}^n (\\frac{\\sigma_k}{\\sigma^{2}_k} + \\frac{(x_{k}^{(i)}-\\mu_{k})^{2}}{\\sigma_{k}^{3}}) \\\\ \n",
    " & =  \\sum_{i=1}^n (\\frac{1}{\\sigma_k} + \\frac{(x_{k}^{(i)}-\\mu_{k})^{2}}{\\sigma_{k}^{3}}) \\\\ \n",
    "  & =  \\frac{n}{\\sigma_k} - \\sum_{i=1}^n \\frac{(x_{k}^{(i)}-\\mu_{k})^{2}}{\\sigma_{k}^{3}} = 0   \\to \\hat{\\sigma_{k}^{2}} = \\frac{1}{n} \\sum_{i=1}^n (x_{k}^{(i)}-\\mu_{k})^{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Ce qui donne bien le résultat suivant pour chaque composante $i$ du vecteur $\\sigma^{2}$ se situant dans la diagonal de la matrice de covariance $\\hat{\\sum}$: \n",
    " $\\hat{\\sigma}_{i}^{2} = \\hat{\\sum_{ii}} = \\frac{1}{n}\\sum_{j=1}^n (x_{i}^{(j)} - \\hat{\\mu_{i}})*(x_{i}^{(j)} - \\hat{\\mu_{i}})$ pour $i \\in \\{1,...,d\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
