{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Fléau de la dimensionalité et intuition géométrique en haute dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calcul du volume d'un hypercube (d-dimensions) de côté c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le volume d'un hypercube (de côté c), appelé aussi hypervolume est défini comme étant l'intégrale sur la région de définition de l'hypercube (c'est-à-dire $d$-intégrales entre [0,c], soit donc une intégrale sur le domaine de définition de chacune des $d$-dimensions). \n",
    "D'un point de vue mathématiques, on a :\n",
    "\n",
    " $$\\int_0^c \\int_0^c ... \\int_0^c 1 \\,dx_1\\,dx_2...\\,dx_d = c^d$$\n",
    " \n",
    "Le volume d'un hypercube (de dimension $d$, en unité $cm^d$) est donc égale à : <font color=blue> $Volume(hypercube) = c^d$ </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Densité de probabilité à l'intérieur de l'hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On appelle densité de probabilité d'une variable aléatoire $X$ à valeur dans $R^d$ une fonction (mathématiques) $f$ telle que pour toute Tribu borélienne : \n",
    "\n",
    "$$\\mathbb{p}(x \\in Region) : \\int_{Region}\\ f_X(x)\\,dx = 1$$ \n",
    "\n",
    "$$\\mathbb{p}(x) : \\int_0^c \\int_0^c ... \\int_0^c f_X(x) \\,dx_1\\,dx_2...\\,dx_d = 1$$\n",
    "$$f_X(x) \\int_0^c \\int_0^c ... \\int_0^c \\,dx_1\\,dx_2...\\,dx_d = 1$$\n",
    "$$f_X(x) * c^d = 1$$\n",
    "<font color=blue> $$f_X(x) = \\frac{1}{c^d}$$ </font>\n",
    "\n",
    "\n",
    "Comme mentionné dans la formule ci-dessus, cela revient donc à faire que : le produit entre ma $pdf$ et l'hypervolume de l'hypercube soit égale à 1 (car la $pdf$ doit toujours être positive et son intégrale doit être égale à 1 $cf$. propriétés des $pdf$). Donc finalement la fonction de densité de probabilité (p(x)) pour l'hypercube est définie de manière à ce qu'elle vaut <font size=4 color=blue> $\\frac{1}{c^d}$ </font>  si $x$ est à l'intérieur du cube, et 0 si $x$ est à l'extérieur du cube.\n",
    "\n",
    "Les propriétés des \"probability density functions\" utilisées pour répondre à cette question sont : \n",
    "$$\\int\\ f_X(x)\\,dx = 1$$ \n",
    "\n",
    "$$f_X(x) \\geq 0$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Densité de probabilité à l'intérieur du petit hypercube et zone de bordure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note notre nouvelle longueur de côté de l'hypercube intérieur comme suit : $k$ = $c$ - $\\frac{2 * 3 * c}{100}$. \n",
    "\n",
    "On commence d'abord par calculer la probabilité que le point tombe dans le petit-hypercube intérieur :\n",
    "$$ \\int_0^k \\int_0^k ... \\int_0^k \\mathbb{p}(x) \\,dx_1\\,dx_2...\\,dx_d = \\mathbb{p}(x) \\int_0^k \\int_0^k ... \\int_0^k \\,dx_1\\,dx_2...\\,dx_d = \\frac{1}{c^d} * k^d $$\n",
    "\n",
    "Où $k^d$ est l'hypervolume du __petit-hypercube__ et <font size=4 color=blue> $\\frac{k^d}{c^d}$ </font> est donc __la probabilité que le point tombe dans ce petit-hypercube__.\n",
    "\n",
    "On peut désormais, calculer la probabilité que le point tombe dans __la zone de bordure__ comme suit : \n",
    "$$\\mathbb{p}(x \\in etroite\\_bordure) = 1-probabilité\\_de\\_tomber\\_dans\\_le\\_petit\\_hypercube = 1 - \\frac{k^d}{c^d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculs numériques de probabilités sur l'étroite bordure selon différentes dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{Rappel}$ : soit $c$ = 100cm (côté du grand hypercube), la zone de bordure étroite est d'une largeur de 3% de $c$ bordant l'hypercube vers l'intérieur, et donc le petit hypercube intérieur est de côté $k$ = $c$ - $\\frac{2 * 3 * c}{100}$ = $100$ - $\\frac{2 * 3 * 100}{100} = 94cm.$\n",
    "\n",
    "* la probabilité pour que $x$ tombe dans le petit hypercube intérieur de dimension d est égale à : $$\\frac{k^d}{c^d} = \\frac{94^d}{100^d}$$\n",
    "* la probabilité pour que $x$ tombe dans l'étroite bordure (zone de bordure) de dimension d est égale à : $$1 - \\frac{k^d}{c^d} = 1 - \\frac{94^d}{100^d}$$\n",
    "\n",
    "+ En dimension d = 1 : $$1 - \\frac{k}{c} = 1 - \\frac{94}{100} = 0.06$$\n",
    "+ En dimension d = 2 : $$1 - \\frac{k^2}{c^2} = 1 - \\frac{94^2}{100^2}=0.116$$\n",
    "+ En dimension d = 3 : $$1 - \\frac{k^3}{c^3} = 1 - \\frac{94^3}{100^3} = 0.169$$\n",
    "+ En dimension d = 5 : $$1 - \\frac{k^5}{c^5} = 1 - \\frac{94^5}{100^5} = 0.266$$\n",
    "+ En dimension d = 10 : $$1 - \\frac{k^{10}}{c^{10}} = 1 - \\frac{94^{10}}{100^{10}} = 0.461$$\n",
    "+ En dimension d = 100 : $$1 - \\frac{k^{100}}{c^{100}} = 1 - \\frac{94^{100}}{100^{100}} = 0.998$$\n",
    "+ En dimension d = 1000 : $$1 - \\frac{k^{1000}}{c^{1000}} = 1 - \\frac{94^{1000}}{100^{1000}} = 1 - \\frac{\\infty}{\\infty}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Que conclure sur la répartition des points en haute dimension qui est contraire à notre intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes bien dans ce qu'on appele le ___\"fléau de la dimensionnalité\"___, i.e. que plus on augmente le nombre de dimensions et plus les points d'une distribution semble se rapprocher des limites de l'espace (de la fine bordure de l'hypercube). Ainsi la probabilité qu'un point tombe dans l'hypercube intérieur est extremement petit; et plus $d\\to\\infty$ et plus la probabilité qu'un point $x$ tombe dans l'étroite bordure devient forte. Cela est illustrée par l'exemple ci-dessus, on voit bien que lorsque le nombre de dimensions $d$ tend vers l'infini alors la probabilité que les points se concentre sur les bordures de l'hypercube tend vers 1.\n",
    "\n",
    "Bien que nous puissions utiliser notre intuition à deux et trois dimensions pour comprendre certains aspects de la géométrie, il existe néanmoins de nombreuses contre-intuitions lorsque nous sommes en haute-dimension (une vaste majorité de la région est vide).\n",
    "\n",
    "Par ailleurs, il est à noter que cette ___\"malédiction de la dimensionnalité\"___ affecte également la capacité à bien généraliser : la difficulté à bien généraliser peut donc potentiellement augmenter exponentiellement avec la dimension \"d\" des entrées. A chaque fois que l'on rajoute une dimension à notre hypercube, on devra peupler ces nouvelles régions induite par la nouvelle dimension; cela nécessite donc de plus en plus de data (croissance exponentielle) pour couvrir toutes les régions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercice 3 : Estimation de densité paramétrique Gaussienne, vs estimation de densité par fenêtres de Parzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimation de densité paramétrique avec une densité Gaussienne isotropique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel une estimation de densité consiste, étant donné un Dataset $D = \\{x^{(1)},...,x^{(n)}\\}$ composé de $n$ exemples et avec $x \\in R^{d}$, à estimer la fonction de densité $p(x)$ ayant pu générer ces données de telle manière que pour chaque nouveau point test $x$ nous pourrons dans le futur calculer $p(x)$. Par ailleurs, il est à noter également que dans le cas d'une Gaussienne isotropique (\"sphérique\") en d-dimensions, la colline de la Gaussienne est centrée en $\\mu$ avec une largeur (écart-type) $\\sigma$ qui est la même dans toutes les directions.\n",
    "\n",
    "a) Les deux paramètres à estimer lors de l'apprentissage sont :\n",
    "- $\\mu$ est un vecteur en dimension $d$ tel que : $\\mu = (\\mu_1,...,\\mu_d)$ qui contient pour chaque $\\mu_j$ la moyenne pondérée de la dimension $x_j$ (et donc de chaque *features*). Concretement, $\\mu$ indique où se trouve le centre (i.e. la moyenne) de ma Gaussienne multivariée (de ma colline) dans mon espace vectoriel (continue) de dimension $d$. Plus $x$ sera proche de $\\mu$ et plus sa densité de probabilité sera élevée. \n",
    "- et la variance $\\sigma^{2}$ qui est un scalaire car nous considérons une *__Gaussienne isotropique__* et donc la variance est la même pour chacune des dimensions (*features*)\n",
    "\n",
    "b) Si on apprend les paramètres de cette Gaussienne isotropique en utilisant le principe de maximum de vraisemblance, la formule qui nous donnera la valeur des paramètres optimaux est : \n",
    "- pour chaque composante $\\mu_j$ de $\\mu$ on aura : $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_j^{(i)}$, sous forme vectorielle (d-dimensions) cela donne : $\\mu = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}$\n",
    "- pour la variance (qui rappelons le est un scalaire) on aura : $\\sigma^{2} = \\frac{1}{n*d}\\sum_{i=1}^n (x^{(i)} - \\mu)^{T}*(x^{(i)} - \\mu)$\n",
    "<br>Une autre manière pour calculer la variance aurait pu être de passer par la matrice de covariance $\\sum = \\frac{1}{n}\\sum_{i=1}^n (x^{(i)} - \\mu)*(x^{(i)} - \\mu)^{T}$ en sachant que tous les $\\sigma^{2}$ en diagonal sont identiques (car Gaussienne isotropique).\n",
    "\n",
    "c) Dans le présent cas, la phase de *training* consiste à apprendre les paramètres $\\mu$ et $\\sigma^{2}$, et donc en terme de complexité algorithmique du calcul de chacun de ces paramètres on aura : \n",
    "- complexité de calcul pour $\\mu$        :  $O(nd)$\n",
    "- complexité de calcul pour $\\sigma^{2}$ :  $O(nd)$\n",
    "\n",
    "d) Pour un nouveau point test $x$, la fonction qui donnera la densité de probabilité prédite au point $x$ est : \n",
    "- $\\hat{p}_{gauss-isotrop}(x) = \\mathcal{N}_{\\mu, \\sigma^{2}}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-\\mu\\|^{2}}{\\sigma^{2}}}$\n",
    "\n",
    "e) La complexité de cette prédiction à chaque nouveau point $x$ est : $O(d)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimation de densité en utilisant des fenêtres de Parzen avec un noyau Gaussien isotropique de largeur (écart-type) $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes à présent dans le cas où nous essayons d'estimé la densité de probabilité en utilisant des fenêtres de Parzen avec un noyau Gaussien istropique (de largeur, écart-type $\\sigma$) et qu'on a entraîné ces fenêtres de Parzen sur $D$. \n",
    "\n",
    "a) Dans le présent cas, la phase \"entrainement/apprentissage\" consiste juste à mémoriser les données d'entrainement (il n'y a pas de paramètres à apprendre). On fixe l'hyperparamètre $\\sigma$ et on mémorise les données. \n",
    "\n",
    "b) Pour cet algorithme de Parzen à noyau Gaussien isotropique, La fonction qui donnera la densité de probabilité prédite au point $x$ est définie comme suit : \n",
    "- $\\hat{p}_{Parzen}(x) = \\frac{1}{n}\\sum_{i=1}^n K(X_{i};x)$ où $K(X_{i};x) = \\mathcal{N}_{X_{i}, \\sigma^{2}}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-X_{i}\\|^{2}}{\\sigma^{2}}}$ est un noyau gaussien isotropique centré en $X_{i}$. \n",
    "___Donc l'estimateur de densité de Parzen se re-écrit comme suit : $\\hat{p}_{Parzen}(x) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sigma^{d}} e^{-\\frac{1}{2}\\frac{\\|x-X_{i}\\|^{2}}{\\sigma^{2}}}$ ___\n",
    "\n",
    "c) La complexité de cette prédiction à chaque nouveau point $x$ est : $O(nd)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Capacité/Expressivité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "a) Avec peu de données, les modèles avec densité paramétrique Gaussienne peuvent bien fonctionner, alors que les estimations avec noyaux nécessitent beaucoup plus de données. L'approche de Parzen est plus souple et plus riche que l'approche paramétrique Gaussienne. Etant donné que $\\sigma$ est considéré comme un hyperparamètre, on peut _tuner_ ce dernier pour avoir différents résultats. \n",
    "\n",
    "b) Dans l'approche avec Parzen à noyau Gaussien, on a un hyperparamètre $\\sigma$ qui est fixé à l'avance et donc si ce dernier ($\\sigma$) est fixé trop élevé par rapport aux données de _training_ alors on aura de grande chance d'être en _sur-apprentissage._ \n",
    "\n",
    "c) Le $\\sigma$ dans les fenêtres de Parzen est traité comme un hyperparamètre, c'est-à-dire qu'il est fixé à l'avance avant l'entrainement car il sera optimisé et ajusté qu'au moyen des ensembes de validation. Et également parce que dans l'estimation de Parzen avec noyau, le $\\sigma$ défini un modèle qui reste inchangé durant l'apprentissage. Le choix du bon hyperparamètre $\\sigma$ sera effectué par un essai de plusieurs valeurs, en déterminant visuellement ou quantitativement la qualité. \n",
    "\n",
    "En revanche, dans le cas de l'estimation de densité paramétrique Gaussienne le $\\sigma$ est considéré comme un paramètre puisqu'il est appris et optimisé à l'apprentissage sur les données d'entrainement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Estimation de densité paramétrique avec une densité Gaussienne diagonale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) L'équation d'une densité Gaussienne diagonale dans $R^{d}$ est définie comme suit : \n",
    "$p_{gauss-diagonal}(x) = \\mathcal{N}_{\\mu, \\sum}(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sqrt{\\mid\\sum\\mid}} e^{-\\frac{1}{2}(x-\\mu)^{T}\\sum^{-1}(x-\\mu)}$\n",
    "\n",
    "Les deux paramètres à estimer lors de l'apprentissage sont :\n",
    "- $\\mu$ (moyenne empirique) est un vecteur colonne en dimension $d$ tel que : $\\mu = (\\mu_1,...,\\mu_d)$ qui contient pour chaque $\\mu_j$ la moyenne pondérée de la dimension $x_j$ (et donc de chaque *features*). Concretement, $\\mu$ indique où se trouve le centre (i.e. la moyenne) de ma Gaussienne multivariée (de ma colline) dans mon espace vectoriel (continue) de dimension $d$. Plus $x$ sera proche de $\\mu$ et plus sa densité de probabilité sera élevée. \n",
    "- et $\\sum$ (covariance empirique) est la matrice de covariance diagonale de dimension $d*d$ definie telle que : \n",
    "\n",
    "$\\sum = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$\n",
    "\n",
    "$\n",
    "\\sum_{ij} = \\begin{cases}\n",
    "\\sigma_{i}^{2}&\\text{si $i=j$}\\\\\n",
    "0&\\text{sinon}\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Le second paramètre (la variance) $\\sigma^{2}$ sera donc un vecteur de dimension $d$ tel que $\\sigma^{2} = (\\sigma^{2}_1,...,\\sigma^{2}_d)$. Tous les $\\sigma_{i}^{2}$ en diagonale sont non-nuls alors que tous les autres élèments (où $i\\neq j$) sont $\\sigma_{i, j}^{2} = 0$\n",
    "\n",
    "b) __A finir de recopier la démo__\n",
    "\n",
    "c) $\\theta^{*} = \\underset{\\theta}{\\operatorname{arg min}} \\hat{R}(P_\\theta, D_{n}) = \\underset{\\theta}{\\operatorname{arg min}} \\frac{1}{n} \\sum_{i=i}^n (-\\log(p(x^{(i)}\\mid\\theta))) = \\underset{\\theta}{\\operatorname{arg min}} \\frac{1}{n} \\sum_{i=i}^n (-\\log(\\frac{1}{(2\\pi)^{\\frac{d}{2}} \\sqrt{\\mid\\sum\\mid}} e^{-\\frac{1}{2}(x-\\mu)^{T}\\sum^{-1}(x-\\mu)})) $\n",
    "\n",
    "d) __Finir de recopier les démos des dérivées partielles__\n",
    "- Moyenne empirique : pour chaque composante $\\mu_j$ de $\\mu$ on aura : $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_j^{(i)}$, sous forme vectorielle (d-dimensions) cela donne : $\\hat{\\mu_{i}} = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}$ \n",
    "\n",
    "- Variance empirique pour chaque composante $\\sigma_{i}^{2}$ de $\\sigma^{2}$ tel que $\\sigma^{2} = (\\sigma_{1}^{2},...,\\sigma_{d}^{2})$ , la variance estimée par le MLE (Maximum Likelihood) se calcule comme suit : $\\sigma_{i_{MLE}}^{2} = \\frac{1}{n}\\sum_{j=1}^n (x_{i}^{(j)} - \\hat{\\mu_{i}})^{T}*(x_{i}^{(j)} - \\hat{\\mu_{i}})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
